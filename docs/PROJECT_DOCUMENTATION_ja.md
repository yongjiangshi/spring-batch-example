# Spring Batch ETLプロジェクト - 技術ドキュメント

## 1. アーキテクチャと設計概要

### 1.1. プロジェクトの目標と範囲
このプロジェクトは、堅牢でスケーラブルなETL（Extract, Transform, Load）バッチ処理パイプラインの実装を目指しています。その中核機能は、CSVファイルから製品データを抽出し、検証と変換を経てリレーショナルデータベースにロードし、その後データベースからデータを抽出・処理して、最終的にCSV形式の販売レポートを生成することです。このプロジェクトは単なる機能実装ではなく、Spring Batchフレームワークのベストプラクティスのショーケースでもあります。

### 1.2. 技術スタック
- **コアフレームワーク:** Spring Boot 3.2, Spring Batch 5.1
- **データ永続化:** Spring Data JPA, Hibernate
- **データベース:** H2 In-Memory Database
- **ビルドツール:** Apache Maven
- **言語:** Java 17

### 1.3. アーキテクチャ設計
プロジェクトは古典的な**マルチステップジョブ（Multi-Step Job）**アーキテクチャを採用し、ETLプロセス全体を2つの独立した順次実行される**ステップ（Step）**に分解しています。この設計は**単一責任の原則**に従っており、各ステップのロジックを凝集させ、理解、テスト、保守を容易にしています。

- **Step 1 (`step1_loadCsvToDb`):** ファイルシステムからデータベースへのデータロードを担当します。
- **Step 2 (`step2_generateReportFromDb`):** データベースからファイルシステムへのデータ抽出とレポート生成を担当します。

ジョブフロー全体は`JobLauncher`によってトリガーされ、その実行状態は`JobRepository`を介して永続化されるため、ジョブの**再起動可能性**と**べき等性**が実現されています。

![Architecture Diagram](https://i.imgur.com/rY8kL0g.png) 
*(これはデータフローを説明する模式図です)*

---

## 2. Spring Batchコアコンポーネントの詳細解析

### 2.1. ジョブ状態の永続化: `JobRepository`
`JobRepository`はSpring Batchの基盤であり、`Job`と`Step`の実行メタデータ（状態、開始/終了時刻、読み書きカウントなど）をデータベースに永続化する責任を負います。このメタデータは通常、`BATCH_`プレフィックスを持つ一連のテーブル（例：`BATCH_JOB_INSTANCE`, `BATCH_STEP_EXECUTION`）に格納されます。

**主な役割:**
- **再起動可能性:** ジョブが予期せず失敗した場合、`JobRepository`に記録された状態により、Spring Batchは次回の実行時に最初からではなく失敗した`Step`から再開でき、成功した操作の再実行を回避します。
- **同時実行制御:** 同じ`JobInstance`（ジョブ名と識別パラメータで定義）が同時に実行されないようにします。

`schema.sql`では`PRODUCTS`テーブルにインデックスを作成していますが、同様に`BATCH_`シリーズのテーブルにインデックスを作成することは、高同時実行下でのバッチジョブのパフォーマンスを向上させる上で極めて重要です。

### 2.2. チャンク指向処理とトランザクション管理
このプロジェクトのすべての`Step`は**チャンク指向処理（Chunk-Oriented Processing）**モデルを採用しています。

```java
// BatchConfiguration.java内
.chunk(10, transactionManager)
```

この設定はトランザクション境界を定義します。そのワークフローは以下の通りです：
1.  **トランザクション開始**。
2.  `ItemReader`が10回呼び出され、10個の`item`を読み取ります。
3.  これら10個の`item`はバッチで`ItemProcessor`に送られ、処理されます。
4.  処理後の`item`のリスト（`Processor`が一部をフィルタリングした場合は10個未満になることがあります）が`ItemWriter`に渡されます。
5.  `ItemWriter`がリスト全体のデータをターゲットに書き込みます。
6.  **トランザクションのコミット**。

このプロセスのいずれかの段階（特に`Writer`）で例外がスローされた場合、トランザクション全体が**ロールバック**され、これら10アイテムのすべての変更が無効になり、データの整合性が保証されます。

---

## 3. プロセス解析: Step 1 - CSV to Database

### 3.1. `FlatFileItemReader`: ファイル読み取り
この`Reader`は`products.csv`の解析を担当します。内部では`DefaultLineMapper`を使用して、ファイルの各行テキストを`Product`オブジェクトにマッピングします。`LineMapper`は通常、2つの部分で構成されます：
- `LineTokenizer`: テキスト行を区切り文字（このプロジェクトではカンマ）でフィールドに分割します。
- `FieldSetMapper`: 分割されたフィールドセット（`FieldSet`）を`Product`オブジェクトのプロパティにマッピングします。

### 3.2. `ProductProcessor`: データ検証と変換
この`Processor`はデータがデータベースに入る前の最後の関門であり、複数の責任を担います：
- **データ検証:** `id`、`name`、`price`などのキーフィールドの有効性をチェックします。無効なデータはフィルタリングされ（`null`を返す）、データベースが「ダーティデータ」で汚染されるのを防ぎます。
- **データクレンジング:** 文字列フィールドに対して`trim()`操作を実行し、データの整形を保証します。
- **データエンリッチメント:** 各レコードに`importDate`タイムスタンプを追加し、インポート時刻を記録します。

### 3.3. `JpaItemWriter`: データベース書き込み
`JpaItemWriter`はJPAの`EntityManager`を利用してデータベース書き込みを実行します。各`item`に対して単純に`INSERT`を実行するわけではありません。代わりに、`chunk`内のすべての`Product`エンティティを現在の永続化コンテキストに`merge`します。トランザクションがコミットされる際、Hibernate/JPAの**バッチ処理メカニズム（JDBC Batching）**がトリガーされ、複数の`INSERT`文を単一のネットワーク呼び出しにまとめてデータベースに送信するため、書き込み性能が大幅に向上します。

---

## 4. プロセス解析: Step 2 - Database to CSV

### 4.1. `JpaPagingItemReader`: スケーラブルなデータベース読み取り
データベースから大量のデータを読み取る際、すべてのデータを一度にメモリにロードするのは非常に危険です。`JpaPagingItemReader`はページングクエリによってこの問題を解決します。
- **動作原理:** `LIMIT`と`OFFSET`（またはデータベース同等の構文）を持つSQLクエリを実行することで、データをバッチで取得します。`read()`呼び出しが現在のページを超えると、自動的に次のページのデータを取得します。
- **`CursorItemReader`との比較:** `Cursor`方式はデータベース接続とカーソルを長時間保持するため、分散環境や高同時実行環境ではリソースの枯渇や接続タイムアウトを引き起こす可能性があります。一方、`Paging`方式はステートレスであり、各クエリが独立しているため、より堅牢でスケーラブルです。

### 4.2. `SalesReportProcessor`: ビジネスフィルタリングとデータ変換
この`Processor`は再びその中核的な役割を示します：
- **ビジネスフィルタリング:** ビジネスルール（`price > 50`）に基づいてデータをフィルタリングします。`null`を返すことは、`ItemProcessor`でフィルタリングを実装する標準的なパターンです。
- **データ変換:** 永続化層の`Product`エンティティを、レポート専用の`SalesReport` **DTO（Data Transfer Object）**に変換します。

**なぜDTOを使用するのか？**
これは重要な設計パターンです。**データ永続化モデル**と**外部インターフェース/レポートモデル**を分離（デカップリング）します。将来、レポートがフィールドを追加・削除したり、フォーマットを変更したりする必要が生じた場合、コアの`Product`エンティティやデータベース構造に触れることなく、`SalesReport` DTOと`Processor`を修正するだけで済みます。

### 4.3. `FlatFileItemWriter`: レポート生成
この`Writer`は`SalesReport` DTOオブジェクトをCSV形式の文字列に変換し、ファイルに書き込む責任を負います。通常、DTOからフィールドを抽出するための`BeanWrapperFieldExtractor`と、それらをカンマで連結して一行のテキストにするための`DelimitedLineAggregator`が設定されます。

---

## 5. 高度な機能の分析

### 5.1. フォールトトレランス: Skip & Retry
`BatchConfiguration`の`.faultTolerant()`は、高度なフォールトトレランス機能を有効にします。
- **`CustomSkipPolicy`:** 「スキップ」ロジックをきめ細かく制御します。例外を2つのカテゴリに分類します：
    - **スキップ可能な例外:** `FlatFileParseException`など。これらの例外は通常、単一の「ダーティ」なレコードによって引き起こされるため、そのレコードをスキップしてジョブを続行すべきです。
    - **致命的な例外:** `TransientDatabaseException`など。これらの例外はシステムレベルの問題を示しており、スキップすべきではなく、リトライをトリガーするか、ジョブを失敗させるべきです。
- **`CustomRetryPolicy`:** **一時的な障害**（データベースのデッドロックやネットワークのジッターなど）に対するリトライ機能を提供します。`spring-retry`ライブラリと統合されており、操作が失敗した後、一定時間遅延（バックオフポリシー）してから再実行します。指定された試行回数内に成功すればジョブは続行し、そうでなければ失敗します。

### 5.2. ジョブ監視: Listeners
`Listener`はSpring Batchの「アスペクト（AOP）」であり、`Job`と`Step`のライフサイクルの重要なポイントでカスタムロジックを注入することができます。
- **`DetailedJobExecutionListener`:** マクロなジョブ監視を提供します。ジョブ完了後、すべてのステップの統計を集計し、総所要時間と成功率を計算し、結果に基づいて最適化の提案（例：高いスキップ率、長い所要時間）を行うことさえできます。
- **`DetailedStepExecutionListener`:** ミクロなパフォーマンス洞察を提供します。各`Step`完了後、そのステップの**処理レート（items/second）**とメモリ使用量を計算し、パフォーマンスのボトルネック分析に重要なデータを提供します。

これらのリスナーが出力するログは、ブラックボックスだったバッチ処理を、完全に観測可能で透明なシステムに変えます。

## 6. まとめとベストプラクティス

このプロジェクトのコードは、本番グレードのSpring Batchアプリケーションを構築するためのいくつかのベストプラクティスを示しています：
1.  **関心の分離:** 読み取り、処理、書き込みのロジックを明確に異なるコンポーネントに分離しています。
2.  **インターフェース指向プログラミング:** `ItemReader`、`ItemProcessor`、`ItemWriter`はすべてインターフェースであり、交換やテストが容易です。
3.  **スケーラブルな設計:** `JpaPagingItemReader`を使用することで、大規模なデータセットを処理する際のパフォーマンスと安定性を確保しています。
4.  **堅牢なエラー処理:** `Skip`および`Retry`戦略を通じて、データレベルおよびシステムレベルの例外を適切に処理しています。
5.  **高い可観測性:** カスタム`Listener`を実装することで、詳細な監視とロギングを実現しています。
6.  **設定の外部化:** バッチパラメータ（`chunkSize`など）を`application.properties`に配置し、異なる環境での調整を容易にしています。
7.  **DTOによるデカップリング:** 内部データモデルと外部データ表現を分離しています。
